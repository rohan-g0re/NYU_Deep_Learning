{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gogyMPbAi6lQ"
   },
   "source": [
    "# Contrastive Language-Image Pre-training\n",
    "\n",
    "This demo is based on the excellent tutorial [here](https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2).\n",
    "\n",
    "In this demo, we implement Contrastive Language-Image Pre-training (CLIP). At a high level, CLIP embeds images and text pairs to a small latent space. The goal is to make the embeddings of image and text pairs similar to each other. In this way, we want the embeddings to capture meaning which *transcends* the difference between language and images.\n",
    "\n",
    "We achieve this goal by encouraging the embedded text and image pair to have high inner product while keeping the inner product of texts and images that are not pairs low.\n",
    "\n",
    "CLIP has been an important component of the break through generative models including Stable Diffusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubXrJTmkzQBp"
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "We'll need a few dependencies to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZSvBQoszjYJS"
   },
   "outputs": [],
   "source": [
    "# !pip install timm\n",
    "# !pip install transformers\n",
    "# !pip install opencv-python\n",
    "# !pip install pandas\n",
    "# !pip install albumentations\n",
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bvXzfQqgi6lT",
    "papermill": {
     "duration": 3.862689,
     "end_time": "2021-04-05T08:01:49.835804",
     "exception": false,
     "start_time": "2021-04-05T08:01:45.973115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18688/3830149982.py:7: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from tqdm.autonotebook import tqdm\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "249mVF6zzV0f"
   },
   "source": [
    "I've been unable to load Flickr8 through pytorch. Instead, we'll use kaggle (we'll need to link a free account to the download)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2y6l5An9jIxI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (1.7.4.2)\n",
      "Requirement already satisfied: bleach in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (3.4)\n",
      "Requirement already satisfied: protobuf in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (6.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (68.2.2)\n",
      "Requirement already satisfied: six>=1.10 in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (4.66.2)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (2.1.0)\n",
      "Requirement already satisfied: webencodings in /home/marvin/miniconda3/envs/hpml/lib/python3.12/site-packages (from kaggle) (0.5.1)\n",
      "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
      "License(s): CC0-1.0\n",
      "flickr8k.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "/bin/bash: line 1: unzip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle --upgrade\n",
    "os.environ['KAGGLE_USERNAME'] = ''\n",
    "os.environ['KAGGLE_KEY'] = ''\n",
    "\n",
    "### For Flickr 8k\n",
    "!kaggle datasets download -d adityajn105/flickr8k\n",
    "!unzip flickr8k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o54bL4gji6lU",
    "papermill": {
     "duration": 0.012732,
     "end_time": "2021-04-05T08:01:51.324144",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.311412",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9ulGHg9ai6lV",
    "papermill": {
     "duration": 0.377383,
     "end_time": "2021-04-05T08:01:51.714313",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.336930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    debug = False\n",
    "    image_path = \"content/Images\"\n",
    "    captions_path = \"content\"\n",
    "    batch_size = 32\n",
    "    num_workers = 2\n",
    "    head_lr = 1e-3\n",
    "    image_encoder_lr = 1e-4\n",
    "    text_encoder_lr = 1e-5\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = 'resnet50'\n",
    "    image_embedding = 2048\n",
    "    text_encoder_model = \"distilbert-base-uncased\"\n",
    "    text_embedding = 768\n",
    "    text_tokenizer = \"distilbert-base-uncased\"\n",
    "    max_length = 200\n",
    "\n",
    "    pretrained = True # for both image encoder and text encoder\n",
    "    trainable = True # for both image encoder and text encoder\n",
    "    temperature = 1.0\n",
    "\n",
    "    # image size\n",
    "    size = 224\n",
    "\n",
    "    # for projection head; used for both image and text encoders\n",
    "    num_projection_layers = 1\n",
    "    projection_dim = 256 \n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM65_Loji6lW",
    "papermill": {
     "duration": 0.012817,
     "end_time": "2021-04-05T08:01:51.802043",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.789226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Whenever we work with text and images, we'll need to do a bit of tedious preprocessing. Here, we'll have to tokenize the sentence descriptions of images before passing them through an embedding and apply a transform to the images before loading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l9V91XcNi6lW",
    "papermill": {
     "duration": 0.025532,
     "end_time": "2021-04-05T08:01:51.840523",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.814991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
    "        \"\"\"\n",
    "        image_filenames and cpations must have the same length; so, if there are\n",
    "        multiple captions for each image, the image_filenames must have repetitive\n",
    "        file names \n",
    "        \"\"\"\n",
    "\n",
    "        self.image_filenames = image_filenames\n",
    "        self.captions = list(captions)\n",
    "        self.encoded_captions = tokenizer(\n",
    "            list(captions), padding=True, truncation=True, max_length=config.max_length\n",
    "        )\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(values[idx])\n",
    "            for key, values in self.encoded_captions.items()\n",
    "        }\n",
    "\n",
    "        image = cv2.imread(f\"{config.image_path}/{self.image_filenames[idx]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transforms(image=image)['image']\n",
    "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
    "        item['caption'] = self.captions[idx]\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrt7aEKm0khg"
   },
   "source": [
    "## Embeddings\n",
    "\n",
    "We'll need a way of encoding the images and text. For the images, we'll use a ResNet to get images down to a latent space of 2048. For the text, we'll use a Bert model to get images down to a latent space of 768.\n",
    "\n",
    "Of course, to compare the embedded texts and images, they'll have to live in the same dimension. We can fix this by adding a projection head on top of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "i0flfMTRi6lY",
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode images to a fixed size vector\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model_name=config.model_name, pretrained=config.pretrained, trainable=config.trainable\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "am5VR4Ezi6lZ",
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=config.text_encoder_model, pretrained=config.pretrained, trainable=config.trainable):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            self.model = DistilBertModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.model = DistilBertModel(config=DistilBertConfig())\n",
    "            \n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = trainable\n",
    "\n",
    "        # we are using the CLS token hidden representation as the sentence's embedding\n",
    "        self.target_token_idx = 0\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.target_token_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "SJbY9Yrui6la",
    "papermill": {
     "duration": 0.027706,
     "end_time": "2021-04-05T08:01:51.907283",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.879577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        projection_dim=config.projection_dim,\n",
    "        dropout=config.dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUs23eeoi6la",
    "papermill": {
     "duration": 0.012961,
     "end_time": "2021-04-05T08:01:51.933336",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.920375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CLIP Architecture\n",
    "\n",
    "Now we're ready to implement the CLIP architecture. As we discussed, we'll first have to embed the texts and images and apply the projection head to get embedded vectors in the same dimension.\n",
    "\n",
    "Once we have the embedded vectors in hand, we'll compute the loss by taking the outer product of the embedded images with the embedded texts. This gives a Gram matrix where entry $(i,j)$ is the inner product between the $i$th text embedding and $j$th image embedding. If we didn't have text or image duplicates, we'd want this matrix to be as close to the identity as possible: entry $(i,i)$ is the inner product between the $i$th image and text pair which we want to be large while entry $(i,j)$ (for $i\\neq j$) should be small.\n",
    "\n",
    "We can encourage this by making our loss the cross entropy loss between the identity matrix and our gram matrix.\n",
    "The picture gets a little bit more complicated when we have duplicates (each image has five different captions). We can deal with this by replacing the identity matrix with a matrix encoding whether different texts (images) are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7MQnmwsWi6lc",
    "papermill": {
     "duration": 0.025366,
     "end_time": "2021-04-05T08:01:51.972338",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.946972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=config.temperature,\n",
    "        image_embedding=config.image_embedding,\n",
    "        text_embedding=config.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        batch_size = len(image_embeddings) \n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v4ZMgz13rgM"
   },
   "source": [
    "Here's an example of what we're doing to get the target matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lgDm0OYYi6lc",
    "outputId": "b83f3844-2bf1-45b9-da6b-b6dc53d7056d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[139.0622, -10.0164,  -1.8822,   2.4144],\n",
      "        [-10.0164, 101.8389,   3.3644,  11.8332],\n",
      "        [ -1.8822,   3.3644, 121.8092,   1.3070],\n",
      "        [  2.4144,  11.8332,   1.3070, 122.4035]])\n",
      "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 1.7096e-43, 8.1475e-40],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# A simple Example\n",
    "\n",
    "batch_size = 4\n",
    "dim = 128\n",
    "embeddings = torch.randn(batch_size, dim)\n",
    "out = embeddings @ embeddings.T\n",
    "print(out)\n",
    "print(F.softmax(out, dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXkzSurfi6ld",
    "papermill": {
     "duration": 0.013097,
     "end_time": "2021-04-05T08:01:51.998635",
     "exception": false,
     "start_time": "2021-04-05T08:01:51.985538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYrHf4yQi6ld"
   },
   "source": [
    "Here are some helper functions to load the training and validation dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HUSC32Cei6ld",
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.012840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_train_valid_dfs():\n",
    "    dataframe = pd.read_csv(f\"{config.captions_path}/captions.csv\")\n",
    "    max_id = dataframe[\"id\"].max() + 1 if not config.debug else 100\n",
    "    image_ids = np.arange(0, max_id)\n",
    "    np.random.seed(42)\n",
    "    valid_ids = np.random.choice(\n",
    "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
    "    )\n",
    "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
    "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
    "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
    "    return train_dataframe, valid_dataframe\n",
    "\n",
    "\n",
    "def build_loaders(dataframe, tokenizer, mode):\n",
    "    transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(config.size, config.size, always_apply=True),\n",
    "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
    "        ]\n",
    "    )\n",
    "    dataset = CLIPDataset(\n",
    "        dataframe[\"image\"].values,\n",
    "        dataframe[\"caption\"].values,\n",
    "        tokenizer=tokenizer,\n",
    "        transforms=transforms,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        shuffle=True if mode == \"train\" else False,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4FcULUYi6le"
   },
   "source": [
    "Here are the standard evaluation and training functions we are by now very familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Jkjp0BEPi6le",
    "papermill": {
     "duration": 0.041512,
     "end_time": "2021-04-05T08:01:52.054352",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.012840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(config.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(config.device) for k, v in batch.items() if k != \"caption\"}\n",
    "        loss = model(batch)\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vBQ2Wuyi6le"
   },
   "source": [
    "Now let's load the dataset into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "d6YpqkNYux7R",
    "outputId": "e9e50eea-6eba-46d5-83a1-32a4ec58a545"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  id  \n",
       "0  A child in a pink dress is climbing up a set o...   0  \n",
       "1              A girl going into a wooden building .   0  \n",
       "2   A little girl climbing into a wooden playhouse .   0  \n",
       "3  A little girl climbing the stairs to her playh...   0  \n",
       "4  A little girl in a pink dress going into a woo...   0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{config.captions_path}/captions.txt\")\n",
    "df['id'] = [id_ for id_ in range(df.shape[0] // 5) for _ in range(5)]\n",
    "df.to_csv(f\"{config.captions_path}/captions.csv\", index=False)\n",
    "df = pd.read_csv(\"captions.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWFNZzs_7z-4"
   },
   "source": [
    "Finally, we'll initialize dataloaders, model, parameter, optimizer, and scheduler. With all this in hand, we can train. Each epoch takes about 9 minutes on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2G3S841Vi6le",
    "papermill": {
     "duration": 16449.265031,
     "end_time": "2021-04-05T12:36:01.333760",
     "exception": false,
     "start_time": "2021-04-05T08:01:52.068729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18688/2525779325.py:18: UserWarning: Argument(s) 'always_apply' are not valid for transform Resize\n",
      "  A.Resize(config.size, config.size, always_apply=True),\n",
      "/tmp/ipykernel_18688/2525779325.py:19: UserWarning: Argument(s) 'always_apply' are not valid for transform Normalize\n",
      "  A.Normalize(max_pixel_value=255.0, always_apply=True),\n"
     ]
    }
   ],
   "source": [
    "train_df, valid_df = make_train_valid_dfs()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n",
    "train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
    "valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "\n",
    "\n",
    "model = CLIPModel().to(config.device)\n",
    "params = [\n",
    "    {\"params\": model.image_encoder.parameters(), \"lr\": config.image_encoder_lr},\n",
    "    {\"params\": model.text_encoder.parameters(), \"lr\": config.text_encoder_lr},\n",
    "    {\"params\": itertools.chain(\n",
    "        model.image_projection.parameters(), model.text_projection.parameters()\n",
    "    ), \"lr\": config.head_lr, \"weight_decay\": config.weight_decay}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", patience=config.patience, factor=config.factor\n",
    ")\n",
    "step = \"epoch\"\n",
    "\n",
    "best_loss = float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185eea98086b4ce4a174333e16d8e2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427c7318d22a4d2d919d34d348527ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model!\n",
      "Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c19e81ead0f4ee991ef869e1a103dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f29bb10e32b4074b9d37f6894180610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Best Model!\n",
      "Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b860bcc55a45dfbaa64d7ab8b71cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8964bb7c6441348041debbcf330ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2214f2204f4a48ba15ff6b32164261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d34daa46dc4c67a2f291bc5b639f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(config.epochs):\n",
    "    print(f\"Epoch: {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = valid_epoch(model, valid_loader)\n",
    "    \n",
    "    if valid_loss.avg < best_loss:\n",
    "        best_loss = valid_loss.avg\n",
    "        torch.save(model.state_dict(), \"best.pt\")\n",
    "        print(\"Saved Best Model!\")\n",
    "    \n",
    "    lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiMjVEVci6le"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "We will test our model by providing it a text and asking for images that are close to it. We'll start by embedding all our images and loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKkDZNP3i6lf"
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(valid_df, model_path):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n",
    "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
    "    \n",
    "    model = CLIPModel().to(config.device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=config.device))\n",
    "    model.eval()\n",
    "    \n",
    "    valid_image_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_loader):\n",
    "            image_features = model.image_encoder(batch[\"image\"].to(config.device))\n",
    "            image_embeddings = model.image_projection(image_features)\n",
    "            valid_image_embeddings.append(image_embeddings)\n",
    "    return model, torch.cat(valid_image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySr-oGpMi6lf"
   },
   "outputs": [],
   "source": [
    "_, valid_df = make_train_valid_dfs()\n",
    "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LF3NabVi6lg"
   },
   "source": [
    "We'll use the code below to embed the text query and look for the images that are closest to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un6RzF4Ri6lg",
    "papermill": {
     "duration": 0.025647,
     "end_time": "2021-04-05T12:36:01.385717",
     "exception": false,
     "start_time": "2021-04-05T12:36:01.360070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(config.text_tokenizer)\n",
    "    encoded_query = tokenizer([query])\n",
    "    batch = {\n",
    "        key: torch.tensor(values).to(config.device)\n",
    "        for key, values in encoded_query.items()\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        text_features = model.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        text_embeddings = model.text_projection(text_features)\n",
    "    \n",
    "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
    "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
    "    \n",
    "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
    "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
    "    \n",
    "    _, axes = plt.subplots(int(np.sqrt(n)), int(np.sqrt(n)), figsize=(10, 10))\n",
    "    for match, ax in zip(matches, axes.flatten()):\n",
    "        image = cv2.imread(f\"{config.image_path}/{match}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(image)\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoOu3JE8i6lg"
   },
   "source": [
    "Now let's see its association abilities in practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "waXVzbioi6lg",
    "outputId": "cfb5164f-5b59-4705-cb9b-06d867b81491"
   },
   "outputs": [],
   "source": [
    "find_matches(model, \n",
    "             image_embeddings,\n",
    "             query=\"a boy jumping with skateboard\",\n",
    "             image_filenames=valid_df['image'].values,\n",
    "             n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "6aea8d84a86a1280a34780710c5e478b9c6b3c016841b907182e7484363820b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
